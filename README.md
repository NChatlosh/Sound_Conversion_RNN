# Sound_Conversion_RNN
This project served as yet another Neural Network experiment in Python using the PyTorch Machine Learning library. The point of this experiment was to see if I could create a network that shifts the pitch and dynamics of a sound to closely match that of another sound.
I attempted to follow along the many tutorials and documentation on the Pytorch website to create a Long Short Term Memory type Recurrent Neural Network. The code is messy as it stands and I plan on scrapping much of it in the future when I give it another shot. At first, I was using simple, short audio samples such as sanre and clap sounds and attempted to train the netowrk to transform samples of my terrible beatboxing into realistic percussion sounds. The interesting part is that this actually worked very well...too well as the resulting audio sounded almost exactly like the samples the network trained on. This told me that the network was overfitting the data. So I decided to experiment some more and used a much larger vocal sample instead this time around. I used a sample of a vocal line from a female vocalist that was recorded in my home studio a while back. I then tried to math her voice as closely as possible and recorded myself singing the phrase, poorly I might add. This time I had to tweak the netowrk quite a bit and was forced to break the audio pieces into much larger batches to speed up the training. I also attempted at this point to experiement with using my graphics card to much of the matrix calculations (hence the .cuda() everywhere in the code). While not exactly what I had anticipated, the results were still very interesting. The network totally warped my voice and you could actually begin to hear the pitch and energy of my voice starting to rise and fall with the pitch of her voice. I created severl revisions and saved all the exported audio, but it came to a point where the tweaks weren't enough to dramatically improve the results. I think my biggest mistake so far has been using stereo audio. This not only doubles the amount of audio data, but also heavily complicates the computations and code. When I revise it, I plan on converting the audio to mono(1 channel) and simplifying things a bit. I also think it would be more beneficial to target speficially the pitch, and allow the network to compute the resulting audio based soley on pitch. I am currently looking at all my options to extract pitch, even creating my own pitch classifier that may come of use in the project. All in all, I learned a ton from this project and can't wait to reboot it with new ideas and code!
